\documentclass[11pt,class=report,crop=false]{standalone}
\usepackage[screen]{../python}

\begin{document}

\setcounter{section}{0}

%====================================================================
\chapter*{Annexe}
%====================================================================
\addcontentsline{toc}{chapter}{Annexe}

\objectifs{On regroupe ici quelques informations diverses et variées.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Une brève histoire des réseaux de neurones}

\begin{itemize}
  \item \textbf{Neurone en biologie.} Entre 1840 et 1900 on découvre que la cellule est l'élément fondamental du vivant ; par observation, Santiago Ram\'on y Cajal et d'autres mettent en évidence que les cellules du système nerveux forment un réseau : les neurones.
  On sait maintenant que le cerveau humain contient entre 80 et 100 milliards de neurones, mais nous avons aussi 500 millions de neurones dans le ventre (soit autant que dans le cerveau d'un chien).
  Un neurone est composé d'un élément central prolongé par un axone au bout duquel se trouvent les synapses. Les neurones sont organisés en réseau, un neurone étant en moyenne relié à $10\,000$ autres neurones et transmet ainsi un signal électrique à tous ses neurones voisins. 
    
  \item \textbf{De la biologie vers l'informatique.} À la fin des années 1940, Donald Hebb  émet l'hypothèse que lors de l'apprentissage certaines connexions synaptiques se renforcent et que ce renforcement persiste. Un modèle mathématique est déduit par Frank Rosenblatt : le perceptron (1958). L'algorithme est mis en \oe uvre dans une machine dédiée à la reconnaissance d'images. 
  Le perceptron, qui correspond à un réseau formé d'un seul neurone, montre très vite ses limites à la fois théoriques (puisqu'il ne peut réaliser le \og{}ou exclusif\fg{}) et aussi en termes de résultats pratiques. Ces deux problèmes sont résolus en considérant des réseaux à plusieurs couches,  mais les calculs à mener sont trop longs à la fois en raison de la puissance des ordinateurs de l'époque et des algorithmes utilisés.  
  
  \item \textbf{Rétropropagation.} Un progrès important est fait grâce l'algorithme de rétropropagation (par Rumelhart, Hinton et Williams en 1986) qui permet un calcul efficace des poids des neurones. On conserve encore aujourd'hui ce principe : des calculs essentiellement numériques, avec un minimum de calculs symboliques au niveau de chaque neurone (afin de calculer la dérivée), le tout avec un grand nombre d'itérations. Des améliorations se succèdent : pooling, dropout, calculs en parallèle, meilleures descentes de gradient.
  
  \item \textbf{Hiver.} Cependant les attentes de l'intelligence artificielle (peut être le nom a-t-il été mal choisi ?) sont largement déçues car ses applications restent limitées. L'intérêt des scientifiques diminue drastiquement. De 1980 à 2000 on parle de l'hiver de l'intelligence artificielle.
  
  \item \textbf{\emph{Deep learning}.} À partir des années 2000 et surtout après 2010 les réseaux de neurones font des progrès fulgurants grâce à l'apprentissage profond. Yann Le Cun démontre l'efficacité des couches de convolution pour la reconnaissance des chiffres. On réalise et entraîne alors des réseaux ayant de plus en plus de couches grâce à des progrès matériels (par exemple le calcul sur les processeurs graphiques GPU) mais surtout grâce aux couches de convolution qui extraient des caractéristiques abstraites des images.  
  
  \item \textbf{ChatGPT}. En 2022, le public découvre ChatGPT avec la génération automatique de textes extrêmement pertinents. Ces modèles de langage sont basés sur des couches d'attention et deviennent vite des outils indispensables au quotidien : explication de concepts, analyse de documents, production de code informatique, etc.
  
  \item \textbf{Présent et avenir.} Les réseaux de neurones s'appliquent à de nombreux domaines : la reconnaissance d'images (par exemple la détection de cancer sur une radiographie), les transports (par exemple la conduite autonome des voitures), les jeux (les ordinateurs battent les champions du monde d'échecs, de go et des jeux vidéos les plus complexes), l'écriture (classement, résumé, traduction)\ldots{} 
  Il persiste cependant une certaine méfiance vis à vis des décisions prises par une machine (sentence de justice, diagnostic médical, publicité ciblée). Une meilleure compréhension du fonctionnement des réseaux de neurones par tous est donc indispensable !
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Références}

\begin{itemize}
  \item \tensorflow{} est développé par \emph{Google}. De nombreux tutoriels sont disponibles pour les débutants :
  \mycenterline{\href{https://www.tensorflow.org/}{tensorflow.org}}
  
  \item \keras{} est maintenant intégré à \tensorflow{} et facilite sa prise en main. 
  Il a été développé par François Cholet auteur du livre \emph{Deep learning with Python} (Manning publications, 2017). Il y a de nombreux exemples d'application :
  \mycenterline{\href{https://keras.io/}{keras.io}}
  
  \item Le livre \emph{Deep learning/L'apprentissage profond} par Goodfellow, Bengio, Courville contient des concepts plus avancés. Il est disponible en anglais et en français. Une version gratuite est consultable ici :
  \mycenterline{\href{https://www.deeplearningbook.org/}{deeplearningbook.org}}
  
  \item Concernant ChatGPT, l'article fondateur est 
  \href{https://arxiv.org/pdf/1706.03762.pdf}{\emph{Attention is all you need}}, des explications mathématiques se trouvent dans
  \href{https://transformer-circuits.pub/2021/framework/index.html}{\emph{A mathematical framework for transformer circuits}}.
  
  \item Vous trouverez des sites qui proposent de tester différents modèles en affichant de façon interactive les tokens et leur probabilité. Par exemple :
  \mycenterline{\href{https://fr.vittascience.com/ia/}{fr.vittascience.com/ia/}}
 
 
  
  \item Un peu de pub pour les livres Exo7 :  
  \begin{itemize}
    \item \emph{Algèbre} et \emph{Analyse} pour toutes les notions de mathématiques niveau première année d'études supérieures,
    \item \emph{Python au lycée} (tome 1 et tome 2) pour apprendre la programmation.
  \end{itemize}
  Ils sont disponibles gratuitement en téléchargement :
  \mycenterline{
  \href{http://exo7.emath.fr/}{exo7.emath.fr} 
  \quad et \quad \href{https://github.com/exo7math/deepmath-exo7}{GitHub : exo7math} 
  }
  et aussi en vente à prix coûtant sur \href{https://www.amazon.fr/}{amazon.fr} en version imprimée.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ce qu'il faut pour utiliser tensorflow/keras}


Vous pouvez récupérer l'intégralité des codes \Python{} ainsi que tous les fichiers sources sur la page \emph{GitHub} d'Exo7 :
\mycenterline{
\href{https://github.com/exo7math/deepmath-exo7}{\og{}GitHub : Deepmath\fg{}}
}

En particulier vous trouvez sur ce site le module \ci{keras_facile} qui vous aide à définir facilement des poids pour un réseau simple.


\medskip

Pour ceux qui souhaitent ne rien installer, il est possible d'utiliser \tensorflow{} en ligne :
\mycenterline{\href{https://colab.research.google.com/notebooks/welcome.ipynb}{Google Colab}}

\medskip


Les activités de ce livre sont écrites pour \Python{} (version 3.12).
Cependant il faut installer un certain nombre de modules complémentaires qui ne sont pas nécessairement présents par défaut. 

\begin{itemize}
  \item \ci{tensorflow} (version 2.18) qui contient le sous-module \ci{keras} (version 3.10),
  \item \ci{numpy} pour les tableaux,
  \item \ci{matplotlib} pour l'affichage de graphiques,
  \item \ci{scipy} pour la convolution,  
  \item \ci{ioimage} pour gérer la lecture et l'écriture d'images.
\end{itemize}  

Pour les chapitres concernant ChatGPT :
\begin{itemize}
	\item \ci{pytorch} (version 2.6) est un équivalent de tensorflow,
	\item \ci{transformers} (version 4.52) gère les LLM,
	\item \ci{ntlk} pour la linguistique,
	\item \ci{bertviz} pour la visualisation des têtes d'attention.
\end{itemize} 

\medskip

Un module s'installe simplement par :
\mycenterline{\ci{pip install mon_module}}

Cependant, il est fortement préférable d'utiliser un gestionnaire d'environnements du type \ci{conda} afin de gérer plusieurs versions de \Python{} et de ses modules.
Voici comment créer et activer un environnement \ci{deepmath} afin d’exécuter les scripts de ce livre :
\mycenterline{\ci{conda create --name deepmath python=3.12 tensorflow keras numpy matplotlib}}

\smallskip

\mycenterline{\ci{conda activate deepmath}}


\medskip

Pour des calculs plus rapides, on profite des processeurs graphiques GPU. Pour cela il faut installer les logiciels \ci{CUDA} et \ci{cuDNN} sur sa machine. Si on souhaite faire les calculs uniquement avec le CPU, on peut ajouter l'instruction suivante en tête du programme : 
\mycenterline{\ci{os.environ["CUDA_VISIBLE_DEVICES"] = "-1"}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Lexique français/anglais}

La plupart des références à propos des réseaux de neurones sont en anglais. Voici donc un petit lexique français/anglais des termes principaux.

\textbf{Réseau de neurones.} \emph{Neural network.}

\begin{itemize}
  \item neurone artificiel/\emph{artificial neuron}
  \item poids/\emph{weights}, biais/\emph{bias}
  \item fonction d'activation/\emph{activation function}
  \item fonction marche de Heaviside/\emph{Heaviside step function}
  \item fonction sigma $\sigma$ ou sigmoïde/\emph{sigmoid function}
  \item fonction d'erreur/\emph{loss function} ou \emph{cost function}
  \item entrée/\emph{input}, sortie/\emph{output}
  \item couches du réseau/\emph{layers}
  \item réseau de neurones avec convolution/\emph{convolutional neural network} abrégé en \emph{cnn}
  \item un motif de convolution s'appelle aussi noyau/\emph{kernel} ou filtre/\emph{filter}
 ou masque/\emph{mask}
  \item \emph{dropout} se traduit par abandon ou décrochage
  \item \emph{pooling} se traduit par regroupement de termes
\end{itemize} 
 
\medskip

\textbf{ChatGPT.} 

\begin{itemize}
	\item grand modèle de langage/\emph{LLM Large Language Model}
	\item jeton/\emph{token}	
	\item plongement/\emph{embedding}
	\item retranscription/\emph{unembedding}
	\item flux résiduel/\emph{residual stream}
	\item sous-bloc d'attention/\emph{attention head}
\end{itemize} 

\medskip
\textbf{Mathématiques.}

\begin{itemize}
  \item dérivée partielle/\emph{partial derivative}
  \item gradient/\emph{gradient} mais la notation anglo-saxonne de $\grad f(x,y)$ est $\nabla f(x,y)$
  \item régression linéaire/\emph{linear regression}
  \item tensor/\emph{tensor}, taille/\emph{shape}, nombre d'éléments/\emph{size}
\end{itemize} 
 
 
\medskip
 
\textbf{Descente de gradient.} \emph{Gradient descent.}

\begin{itemize}
 \item descente de gradient (classique)/\emph{(batch) gradient descent}
 \item descente de gradient stochastique/\emph{stochastic gradient descent} abrégée en \emph{sgd}
 \item descente de gradient par lots/\emph{mini-batch gradient descent}
 \item pas $\delta$/\emph{learning rate}
 \item erreur quadratique moyenne/\emph{minimal squared error} abrégée en \emph{mse}
 \item moment/\emph{momentum}
 \item époque/\emph{epoch}
\end{itemize}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L'image de couverture}

L'image de couverture du livre est générée automatiquement grâce à un réseau de neurones et  la rétropropagation. Au départ il faut deux images :
\begin{itemize}
  \item une image dont on veut conserver le contenu :  ici la statue de la liberté,
  \item et une image dont on veut conserver le style : ici un tableau de Van Gogh.
\end{itemize}
L'image obtenue possède le contenu de la première et le style de la seconde !

\begin{center}
\includegraphics[scale=\myscale,scale=0.15]{figures/statue-of-liberty_resized.jpg}
\qquad\qquad
\includegraphics[scale=\myscale,scale=0.18]{figures/Van_Gogh_nuit.jpg}
\end{center}

Références :
\begin{itemize}
  \item Statue de la liberté (photographie par George Hodan, licence CC0, domaine public).
  \item Peinture de Vincent Van Gogh, \emph{La nuit étoilée}, 1889.
  \item Le programme et les explications pour \tensorflow{} :  \href{https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb}{\emph{Neural Style Transfer} avec TensorFlow} d'après des travaux de Gatys, Ecker et Bethge.
\end{itemize}

\begin{center}
\includegraphics[scale=\myscale,scale=0.5]{figures/iter_500it_256px.png}
\end{center}



Plus en détails :
\begin{itemize}
  \item le réseau de neurones, nommé \og{}VGG19\fg{}, est un réseau à 19 couches. 
  On ne va pas calculer les poids, mais on utilise des poids qui ont déjà été calculés pour reconnaître les images de la base \emph{ImageNet}.
  
  \item Dans ce réseau bien paramétré certaines couches se concentrent sur le contenu (contour, lignes, formes\ldots) et d'autres se concentrent sur le style (couleur, flou\ldots).
  
  \item En identifiant ces couches, on construit par itérations une image qui conserve le contenu de la première image et prend peu à peu le style de la seconde.
  
  \item Ces itérations se font par rétropropagation et descente de gradient selon une fonction d'erreur qui est la somme d'une fonction d'erreur associée au contenu et d'une fonction d'erreur associée au style.
 
\end{itemize}


\begin{center}
\includegraphics[scale=\myscale,scale=0.5]{figures/best_1000it_512px.png}
\end{center}


\end{document}
